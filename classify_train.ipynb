{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%set_env CUDA_VISIBLE_DEVICES=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification models training\n",
    "\n",
    "Use this notebook to train classification models (KNN, SVM, etc) on leaf color classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data gathering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use pre-annotated dataset with each leaf segmentation and class\n",
    "# From the data.yaml of this dataset, the label number to corresponding class is:\n",
    "# 0=dark, 1=dead, 2=light, 3=medium\n",
    "\n",
    "# This creates a list called 'obj_data', which containg every object as a tuple...\n",
    "# ...containing (obj_classnum, obj_crop)\n",
    "\n",
    "from typing import List\n",
    "import scg_detection_tools.utils.image_tools as imtools\n",
    "import scg_detection_tools.utils.cvt as cvt\n",
    "from scg_detection_tools.utils.file_handling import get_all_files_from_paths, get_annotation_files\n",
    "from scg_detection_tools.dataset import read_dataset_annotation\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "IMG_DIR = \"/home/juliocesar/leaf-detection/imgs/light_group/images\"\n",
    "LBL_DIR = \"/home/juliocesar/leaf-detection/imgs/light_group/labels\"\n",
    "\n",
    "#IMG_DIR = \"/home/juliocesar/leaf-detection/imgs/hemacias/annotated/images\"\n",
    "#LBL_DIR = \"/home/juliocesar/leaf-detection/imgs/hemacias/annotated/labels\"\n",
    "\n",
    "imgs = get_all_files_from_paths(IMG_DIR, skip_ext=[\".txt\", \".json\", \".yaml\"])\n",
    "\n",
    "# CHOOSING 32x32 because of calculated average\n",
    "STANDARD_SIZE = (32, 32)\n",
    "MAX_MEDIUM_RATIO = 0.30\n",
    "\n",
    "# !!!!!! taken from data.yaml\n",
    "class_map = {0: \"dark\", 1: \"dead\", 2: \"light\", 3: \"medium\"}\n",
    "#class_map = {0: \"purple\", 1: \"white\"}\n",
    "\n",
    "def extract_objects_from_annotations(imgs: List[str], ann_dir: str, std_size=(32,32), max_cls_ratio: dict[int, float] = None, use_boxes=False):\n",
    "    img_ann = get_annotation_files(imgs, ann_dir)\n",
    "    obj_data = []\n",
    "    sample_count = {cls: 0 for cls in class_map}\n",
    "    for img in imgs:\n",
    "        ann_file = img_ann[img]\n",
    "        annotations = read_dataset_annotation(ann_file, separate_class=False)\n",
    "\n",
    "        # check if contours are boxes or segments\n",
    "        orig = cv2.imread(img)\n",
    "        orig = cv2.cvtColor(orig, cv2.COLOR_BGR2RGB)\n",
    "        imgsz = orig.shape[:2]\n",
    "\n",
    "        for ann in annotations:\n",
    "            nclass = ann[0]\n",
    "            if nclass in max_cls_ratio:\n",
    "                if (len(obj_data) >= 1) and ((sample_count[nclass]/len(obj_data)) >= max_cls_ratio[nclass]):\n",
    "                    continue\n",
    "            sample_count[nclass] += 1\n",
    "\n",
    "            contour = ann[1:]\n",
    "            if use_boxes and len(contour) != 4:\n",
    "                contour = cvt.segment_to_box(contour, normalized=True, imgsz=imgsz)\n",
    "            else:\n",
    "                mask = cvt.contours_to_masks([contour], imgsz=imgsz, normalized=True)[0]\n",
    "            \n",
    "            # get only segmented object from image\n",
    "            if not use_boxes:\n",
    "                masked = orig.copy()\n",
    "                masked[mask[:,:] < 255] = 0\n",
    "\n",
    "                # crop a box around it\n",
    "                points = np.array(contour).reshape(len(contour) // 2, 2)\n",
    "                box = cvt.segment_to_box(points, normalized=True, imgsz=imgsz)\n",
    "                obj_crop = imtools.crop_box_image(masked, box)\n",
    "            else:\n",
    "                obj_crop = imtools.crop_box_image(orig, contour)\n",
    "\n",
    "            # resize to 32x32 and add to our data\n",
    "            obj_crop = cv2.resize(obj_crop, std_size, cv2.INTER_CUBIC)\n",
    "            obj_data.append((nclass, obj_crop))\n",
    "    return obj_data\n",
    "\n",
    "obj_data = extract_objects_from_annotations(imgs, LBL_DIR, STANDARD_SIZE, max_cls_ratio={3: 0.3}, use_boxes=True)\n",
    "ncls = [obj[0] for obj in obj_data]\n",
    "for cls in np.unique(ncls):\n",
    "    print(f\"Samples of type {cls}: {class_map[cls]!r} = {len([c for c in ncls if c == cls])}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split between Train and Test to evaluate model as well\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = []\n",
    "y = []\n",
    "for nclass, obj_crop in obj_data:\n",
    "    X.append(obj_crop)\n",
    "    y.append(nclass)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=0)\n",
    "class_labels = [class_map[c] for c in class_map]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing functions (to be able to call clf.predict(imgs) instead of having to extract features first and then calling clf.predict(features))\n",
    "# -> rn_feature_preprocess: use resnet feature extraction to train classificators\n",
    "# -> channels_feature_preprocess: extract RGB, HSV and Gray values from a 32x32 image as features\n",
    "def rn18_feature_preprocess(objX):\n",
    "    \"\"\"\n",
    "    Extract 512-dimensional vector of features from ResNet34\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "\n",
    "    if not isinstance(objX[0], np.ndarray):\n",
    "        raise TypeError(\"'objX' passed to preprocess function must be a list of np.ndarray RGB images\")\n",
    "\n",
    "    from object_detection_analysis.classifiers import resnet_extract_features\n",
    "    processed = []\n",
    "    for obj in objX:\n",
    "        processed.append(resnet_extract_features(obj, resnet=18))\n",
    "    return np.array(processed)\n",
    "\n",
    "def rn34_feature_preprocess(objX):\n",
    "    \"\"\"\n",
    "    Extract 512-dimensional vector of features from ResNet34\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "\n",
    "    if not isinstance(objX[0], np.ndarray):\n",
    "        raise TypeError(\"'objX' passed to preprocess function must be a list of np.ndarray RGB images\")\n",
    "\n",
    "    from object_detection_analysis.classifiers import resnet_extract_features\n",
    "    processed = []\n",
    "    for obj in objX:\n",
    "        processed.append(resnet_extract_features(obj, resnet=34))\n",
    "    return np.array(processed)\n",
    "\n",
    "def rn50_feature_preprocess(objX):\n",
    "    \"\"\"\n",
    "    Extract 512-dimensional vector of features from ResNet50\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "\n",
    "    if not isinstance(objX[0], np.ndarray):\n",
    "        raise TypeError(\"'objX' passed to preprocess function must be a list of np.ndarray RGB images\")\n",
    "\n",
    "    from object_detection_analysis.classifiers import resnet_extract_features\n",
    "    processed = []\n",
    "    for obj in objX:\n",
    "        processed.append(resnet_extract_features(obj, resnet=50))\n",
    "    return np.array(processed)\n",
    "\n",
    "def channels_feature_preprocess(objX):\n",
    "    \"\"\"\n",
    "    Extract RGB, HSV and Gray channels from objects.\n",
    "    \"\"\"\n",
    "    import cv2\n",
    "    import numpy as np\n",
    "\n",
    "    if not isinstance(objX[0], np.ndarray):\n",
    "        raise TypeError(\"'objX' passed to preprocess function must be a list of np.ndarray RGB images\")\n",
    "\n",
    "    processed = []\n",
    "    for obj in objX:\n",
    "        rgb = cv2.resize(obj, (32,32))\n",
    "        hsv = cv2.cvtColor(rgb, cv2.COLOR_RGB2HSV)\n",
    "        gray = cv2.cvtColor(rgb, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "        attributes = np.concatenate((rgb.flatten(), hsv.flatten(), gray.flatten()))\n",
    "        processed.append(attributes)\n",
    "\n",
    "    return np.array(processed)\n",
    "\n",
    "def norm_channels_feature_preprocess(objX):\n",
    "    \"\"\"\n",
    "    Extract RGB, HSV and Gray channels from objects, normalize each feature vector and then use PCA to reduce dimensionality to 256 features.\n",
    "    \"\"\"\n",
    "    import cv2\n",
    "    import numpy as np\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    from sklearn.decomposition import PCA\n",
    "    from sklearn.pipeline import Pipeline\n",
    "    \n",
    "    if not isinstance(objX[0], np.ndarray):\n",
    "        raise TypeError(\"'objX' passed to preprocess function must be a list of np.ndarray RGB images\")\n",
    "\n",
    "    processed = []\n",
    "    for obj in objX:\n",
    "        rgb = cv2.resize(obj, (32,32))\n",
    "        hsv = cv2.cvtColor(rgb, cv2.COLOR_RGB2HSV)\n",
    "        gray = cv2.cvtColor(rgb, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "        attributes = np.concatenate((rgb.flatten(), hsv.flatten(), gray.flatten()))\n",
    "        processed.append(attributes)\n",
    "    processed = np.array(processed)\n",
    "    # mean = processed.mean(axis=0, keepdims=True)\n",
    "    # std = processed.std(axis=0, keepdims=True)\n",
    "    # norm = (processed - mean) / (std + 1e-7)\n",
    "\n",
    "    norm_pca_pipe = Pipeline(\n",
    "        steps = [\n",
    "            (\"scaler\", StandardScaler()), \n",
    "            #(\"pca\", PCA(n_components=256))\n",
    "        ]\n",
    "    )\n",
    "    norm = norm_pca_pipe.fit_transform(processed)\n",
    "    return norm.astype(np.float32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters test training (e.g. optimal k value for KNN, loss for SGD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################\n",
    "############### K VALUE TEST FOR KNN #################\n",
    "######################################################\n",
    "\n",
    "################ CHANNELS FEATURE EXTRACTION\n",
    "### LAST TESTED OPTIMAL K=2 (no nca)\n",
    "\n",
    "from object_detection_analysis.classifiers import KNNClassifier\n",
    "\n",
    "MAX_K = 25\n",
    "for k in range(1, MAX_K+1):\n",
    "    knn = KNNClassifier(n_neighbors=k, enable_nca=False, preprocess=channels_feature_preprocess)\n",
    "    knn.fit(X_train, y_train)\n",
    "    \n",
    "    print(\"_\"*82)\n",
    "    print(f\"EVALUATION: K = {k}\")\n",
    "    print(\"_\"*82)\n",
    "    knn.evaluate(X_test, y_test, disp_labels=class_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################ RESNET FEATURE EXTRACTION\n",
    "### BEST FOR RESNET18: K=2\n",
    "### BEST FOR RESNET34: NONE\n",
    "### BEST FOR RESNET50: NONE\n",
    "\n",
    "from object_detection_analysis.classifiers import KNNClassifier\n",
    "\n",
    "MAX_K = 15\n",
    "for k in range(1, MAX_K+1):\n",
    "    knn = KNNClassifier(n_neighbors=k, preprocess=rn34_feature_preprocess, enable_nca=False)\n",
    "    knn.fit(X_train, y_train)\n",
    "    \n",
    "    print(\"_\"*82)\n",
    "    print(f\"EVALUATION: K = {k}\")\n",
    "    print(\"_\"*82)\n",
    "    knn.evaluate(X_test, y_test, disp_labels=class_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGD LOSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################\n",
    "################# LOSS TEST FOR SGD ##################\n",
    "######################################################\n",
    "\n",
    "######### CHANNELS FEATURE EXTRACTION\n",
    "# BEST TEST: LOG_LOSS\n",
    "\n",
    "from object_detection_analysis.classifiers import SGDBasedClassifier\n",
    "\n",
    "LOSS_FN = [\"hinge\", \"log_loss\", \"modified_huber\", \"squared_hinge\", \"perceptron\", \"squared_error\", \"huber\", \"epsilon_insensitive\", \"squared_epsilon_insensitive\"]\n",
    "for loss in LOSS_FN:\n",
    "    sgd = SGDBasedClassifier(loss=loss, max_iter=1000, preprocess=channels_feature_preprocess)\n",
    "    sgd.fit(X_train, y_train)\n",
    "\n",
    "    print(\"_\"*82)\n",
    "    print(f\"EVALUATION: loss = {loss!r}\")\n",
    "    print(\"_\"*82)\n",
    "    sgd.evaluate(X_test, y_test, disp_labels=class_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######### RESNET18 FEATURE EXTRACTION\n",
    "# BEST TEST: HINGE\n",
    "\n",
    "from object_detection_analysis.classifiers import SGDBasedClassifier\n",
    "\n",
    "#LOSS_FN = [\"hinge\", \"log_loss\", \"modified_huber\", \"squared_hinge\", \"perceptron\", \"squared_error\", \"huber\", \"epsilon_insensitive\", \"squared_epsilon_insensitive\"]\n",
    "LOSS_FN = [\"hinge\"]\n",
    "for loss in LOSS_FN:\n",
    "    sgd = SGDBasedClassifier(loss=loss, max_iter=1000, preprocess=rn34_feature_preprocess)\n",
    "    sgd.fit(X_train, y_train)\n",
    "\n",
    "    print(\"_\"*82)\n",
    "    print(f\"EVALUATION: loss = {loss!r}\")\n",
    "    print(\"_\"*82)\n",
    "    sgd.evaluate(X_test, y_test, disp_labels=class_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actual training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################\n",
    "##### TRAIN KNN WITH RESNET FEATURE EXTRACTION #####\n",
    "#####################################################\n",
    "\n",
    "from object_detection_analysis.classifiers import KNNClassifier\n",
    "\n",
    "# LEAF CLASSIFICATION: RESNET18-K=2, RESNET34-K=None, RESNET50-K=None\n",
    "# BLOOD CELL CLASSIFICATION: K = ?\n",
    "\n",
    "resnet_knn = KNNClassifier(n_neighbors=2, preprocess=rn18_feature_preprocess)\n",
    "resnet_knn.fit(X, y)\n",
    "#resnet_knn.fit(X_train, y_train)\n",
    "#resnet_knn.evaluate(X_test, y_test, disp_labels=class_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet_knn.save_state(\"knn_rn18_k2_box.skl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################\n",
    "##### TRAIN KNN WITH MANUAL CHANNELS FEATURE EXTRACTION #####\n",
    "#############################################################\n",
    "\n",
    "from object_detection_analysis.classifiers import KNNClassifier\n",
    "\n",
    "# LEAF CLASSIFICATION: K = 2\n",
    "# BCELL CLASSIFICATION: K = 5\n",
    "\n",
    "knn = KNNClassifier(n_neighbors=2, preprocess=channels_feature_preprocess)\n",
    "knn.fit(X, y)\n",
    "#knn.fit(X_train, y_train)\n",
    "#knn.evaluate(X_test, y_test, disp_labels=class_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn.save_state(\"knn_k2_box.skl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################\n",
    "##### TRAIN SVM WITH RESNET FEATURE EXTRACTION #####\n",
    "####################################################\n",
    "\n",
    "# BEST WITH RESNET18\n",
    "\n",
    "from object_detection_analysis.classifiers import SVMClassifier\n",
    "\n",
    "sv = SVMClassifier(preprocess=rn18_feature_preprocess)\n",
    "sv.fit(X, y)\n",
    "#sv.fit(X_train, y_train)\n",
    "#sv.evaluate(X_test, y_test, disp_labels=class_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "sv.save_state(\"svm_rn18_box.skl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################\n",
    "##### TRAIN SVM WITH MANUAL CHANNEL FEATURE EXTRACTION #####\n",
    "############################################################\n",
    "\n",
    "from object_detection_analysis.classifiers import SVMClassifier\n",
    "\n",
    "sv = SVMClassifier(preprocess=channels_feature_preprocess)\n",
    "sv.fit(X, y)\n",
    "#sv.fit(X_train, y_train)\n",
    "#sv.evaluate(X_test, y_test, disp_labels=class_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "sv.save_state(\"svm_box.skl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################\n",
    "##### TRAIN SGD WITH MANUAL CHANNEL FEATURE EXTRACTION #####\n",
    "############################################################\n",
    "\n",
    "from object_detection_analysis.classifiers import SGDBasedClassifier\n",
    "\n",
    "sgd = SGDBasedClassifier(loss=\"hinge\", preprocess=channels_feature_preprocess, max_iter=5000)\n",
    "sgd.fit(X, y)\n",
    "#sgd.fit(X_train, y_train)\n",
    "#sgd.evaluate(X_test, y_test, disp_labels=class_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd.save_state(\"sgd.skl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################\n",
    "##### TRAIN SGD WITH RESNET FEATURE EXTRACTION #####\n",
    "######################################################\n",
    "\n",
    "## BEST RESULTS WITH RESNET34\n",
    "\n",
    "from object_detection_analysis.classifiers import SGDBasedClassifier\n",
    "\n",
    "sgd = SGDBasedClassifier(loss=\"hinge\", preprocess=rn34_feature_preprocess, max_iter=1000)\n",
    "sgd.fit(X, y)\n",
    "#sgd.fit(X_train, y_train)\n",
    "#sgd.evaluate(X_test, y_test, disp_labels=class_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd.save_state(\"sgd_rn34.skl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################\n",
    "##### TRAIN MLP WITH RESNET FEATURE EXTRACTION #####\n",
    "####################################################\n",
    "\n",
    "#### BEST RESULTS: RESNET34\n",
    "\n",
    "from object_detection_analysis.classifiers import MLPClassifier\n",
    "\n",
    "RN = [rn18_feature_preprocess, rn34_feature_preprocess, rn50_feature_preprocess]\n",
    "RNID = [18, 34, 50]\n",
    "\n",
    "for id, func in zip(RNID, RN):\n",
    "    rn_out_features = 512 if id != 50 else 2048\n",
    "    mlp = MLPClassifier(n_features=rn_out_features, n_classes=len(class_map), preprocess=func)\n",
    "    \n",
    "    mlp.fit(X, y, epochs=50)\n",
    "    mlp.save_state(f\"mlp_rn{id}_box.pt\")\n",
    "    \n",
    "    # print(\"_\"*82, \"\\nRESNET:\", id, \"\\n\", \"_\"*82)\n",
    "    # mlp.fit(X_train, y_train, epochs=50)\n",
    "    # mlp.evaluate(X_test, y_test, disp_labels=class_labels)\n",
    "    # print(\"_\"*82)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################\n",
    "##### TRAIN MLP WITH NORMALIZED CHANNELS FEATURE EXTRACTION #####\n",
    "#################################################################\n",
    "\n",
    "from object_detection_analysis.classifiers import MLPClassifier\n",
    "\n",
    "n_features = 32*32*(3 + 3 + 1) # 32x32 leaf RGB, HSV and Gray\n",
    "mlp = MLPClassifier(n_features=n_features, n_classes=len(class_map), preprocess=norm_channels_feature_preprocess)\n",
    "\n",
    "mlp.fit(X, y, epochs=50)\n",
    "#mlp.fit(X_train, y_train, epochs=70)\n",
    "#mlp.evaluate(X_test, y_test, disp_labels=class_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp.save_state(\"mlp.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking saved states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################################################\n",
    "## CELLS BELOW ARE FOR CHECKING SAVED MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from object_detection_analysis.classifiers import KNNClassifier\n",
    "\n",
    "knn = KNNClassifier.from_state(\"/home/juliocesar/leaf-detection/checkpoints/classifiers/knn_k4.skl\")\n",
    "knn.evaluate(X_test, y_test, disp_labels=class_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from object_detection_analysis.classifiers import SVMClassifier\n",
    "\n",
    "svm = SVMClassifier.from_state(\"/home/juliocesar/leaf-detection/checkpoints/classifiers/svm.skl\")\n",
    "svm.evaluate(X_test, y_test, disp_labels=class_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from object_detection_analysis.classifiers import SGDBasedClassifier\n",
    "\n",
    "sgd = SGDBasedClassifier.from_state(\"/home/juliocesar/leaf-detection/checkpoints/classifiers/sgd.skl\")\n",
    "sgd.evaluate(X_test, y_test, disp_labels=class_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from object_detection_analysis.classifiers import MLPClassifier\n",
    "\n",
    "mlp = MLPClassifier.from_state(\"/home/juliocesar/leaf-detection/checkpoints/classifiers/mlp.pt\")\n",
    "mlp.evaluate(X_test, y_test, disp_labels=class_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cls, test_obj = obj_data[9]\n",
    "print(test_cls, test_obj.shape)\n",
    "\n",
    "mlp.eval()\n",
    "print(mlp.predict([test_obj]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "leaf-detection",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
