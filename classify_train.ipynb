{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%set_env CUDA_VISIBLE_DEVICES=2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification models training\n",
    "\n",
    "Use this notebook to train classification models (KNN, SVM, etc) on leaf color classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data gathering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use pre-annotated dataset with each leaf segmentation and class\n",
    "# From the data.yaml of this dataset, the label number to corresponding class is:\n",
    "# 0=dark, 1=dead, 2=light, 3=medium\n",
    "\n",
    "# This creates a list called 'obj_data', which contains every object as a tuple...\n",
    "# ...containing (obj_classnum, obj_crop)\n",
    "\n",
    "from typing import List\n",
    "import scg_detection_tools.utils.image_tools as imtools\n",
    "import scg_detection_tools.utils.cvt as cvt\n",
    "from scg_detection_tools.utils.file_handling import get_all_files_from_paths, get_annotation_files\n",
    "from scg_detection_tools.dataset import read_dataset_annotation\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "IMG_DIR = \"/home/juliocesar/leaf-detection/imgs/light_group/images\"\n",
    "LBL_DIR = \"/home/juliocesar/leaf-detection/imgs/light_group/labels\"\n",
    "\n",
    "#IMG_DIR = \"/home/juliocesar/leaf-detection/imgs/hemacias/annotated/images\"\n",
    "#LBL_DIR = \"/home/juliocesar/leaf-detection/imgs/hemacias/annotated/labels\"\n",
    "\n",
    "imgs = get_all_files_from_paths(IMG_DIR, skip_ext=[\".txt\", \".json\", \".yaml\"])\n",
    "\n",
    "STANDARD_SIZE = (32, 32)\n",
    "MAX_MEDIUM_RATIO = 0.30\n",
    "\n",
    "# !!!!!! taken from data.yaml\n",
    "class_map = {0: \"dark\", 1: \"dead\", 2: \"light\", 3: \"medium\"}\n",
    "#class_map = {0: \"purple\", 1: \"white\"}\n",
    "\n",
    "sizes = []\n",
    "\n",
    "def extract_objects_from_annotations(imgs: List[str], ann_dir: str, std_size=(32,32), max_cls_ratio: dict[int, float] = None, use_boxes=False):\n",
    "    img_ann = get_annotation_files(imgs, ann_dir)\n",
    "    obj_data = []\n",
    "    sample_count = {cls: 0 for cls in class_map}\n",
    "    for img in imgs:\n",
    "        ann_file = img_ann[img]\n",
    "        annotations = read_dataset_annotation(ann_file, separate_class=False)\n",
    "\n",
    "        # check if contours are boxes or segments\n",
    "        orig = cv2.imread(img)\n",
    "        orig = cv2.cvtColor(orig, cv2.COLOR_BGR2RGB)\n",
    "        imgsz = orig.shape[:2]\n",
    "\n",
    "        for ann in annotations:\n",
    "            nclass = ann[0]\n",
    "            if (max_cls_ratio is not None) and (nclass in max_cls_ratio):\n",
    "                if (len(obj_data) >= 1) and ((sample_count[nclass]/len(obj_data)) >= max_cls_ratio[nclass]):\n",
    "                    continue\n",
    "            sample_count[nclass] += 1\n",
    "\n",
    "            contour = ann[1:]\n",
    "            if use_boxes and len(contour) != 4:\n",
    "                contour = cvt.segment_to_box(contour, normalized=True, imgsz=imgsz)\n",
    "            else:\n",
    "                mask = cvt.contours_to_masks([contour], imgsz=imgsz, normalized=True)[0]\n",
    "                mask = mask.reshape(mask.shape[0], mask.shape[1], 1)\n",
    "            \n",
    "            # get only segmented object from image\n",
    "            if not use_boxes:\n",
    "                masked = orig.copy()\n",
    "                # masked[mask[:,:] < 1] = 0\n",
    "                masked = masked * mask.astype(masked.dtype)\n",
    "\n",
    "                # crop a box around it\n",
    "                points = np.array(contour).reshape(len(contour) // 2, 2)\n",
    "                box = cvt.segment_to_box(points, normalized=True, imgsz=imgsz)\n",
    "                obj_crop = imtools.crop_box_image(masked, box)\n",
    "            else:\n",
    "                obj_crop = imtools.crop_box_image(orig, contour)\n",
    "\n",
    "            # resize to 32x32 and add to our data\n",
    "            obj_crop = cv2.resize(obj_crop, std_size, interpolation=cv2.INTER_CUBIC)\n",
    "            obj_data.append((nclass, obj_crop))\n",
    "    return obj_data\n",
    "\n",
    "obj_data = extract_objects_from_annotations(imgs, LBL_DIR, STANDARD_SIZE, max_cls_ratio=None, use_boxes=False)\n",
    "ncls = [obj[0] for obj in obj_data]\n",
    "for cls in np.unique(ncls):\n",
    "    print(f\"Samples of type {cls}: {class_map[cls]!r} = {len([c for c in ncls if c == cls])}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test data loaded\n",
    "idx = 100\n",
    "plt.title(class_map[obj_data[idx][0]])\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(obj_data[idx][1].astype(np.int32))\n",
    "print(obj_data[idx][1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split between Train and Test to evaluate model as well\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = []\n",
    "y = []\n",
    "for nclass, obj_crop in obj_data:\n",
    "    X.append(obj_crop)\n",
    "    y.append(nclass)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=0)\n",
    "class_labels = [class_map[c] for c in class_map]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing functions (to be able to call clf.predict(imgs) instead of having to extract features first and then calling clf.predict(features))\n",
    "# -> rn_feature_preprocess: use resnet feature extraction to train classificators\n",
    "# -> channels_feature_preprocess: extract RGB, HSV and Gray values from a 32x32 image as features\n",
    "\n",
    "from object_detection_analysis.classifiers.feature_extract import (\n",
    "    rn18_feature_preprocess, rn34_feature_preprocess, rn50_feature_preprocess,\n",
    "    channels_feature_preprocess, norm_channels_feature_preprocess, norm_image_to_tensor\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize ResNet feature extraction with PCA\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "colors=[\"red\",\"black\",\"deepskyblue\",\"green\"]\n",
    "labels=[\"dark\",\"dead\",\"light\",\"green\"]\n",
    "def test_pca(objs, tgts):\n",
    "    rn_features = rn50_feature_preprocess(objs)\n",
    "    pca = PCA(n_components=2)\n",
    "    transX = pca.fit_transform(rn_features)\n",
    "\n",
    "    fig, ax = plt.subplots(layout=\"tight\")\n",
    "    for i in range(len(transX)):\n",
    "        ax.scatter(transX[i][0],transX[i][1],c=colors[tgts[i]])\n",
    "    ax.set(xlabel=\"PCA component 0\", ylabel=\"PCA component 1\")\n",
    "    plt.show()\n",
    "\n",
    "test_pca(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters test training (e.g. optimal k value for KNN, loss for SGD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############### K VALUE TEST FOR KNN WITH MANUAL CHANNELS FEATURE EXTRACTION #################\n",
    "\n",
    "################ CHANNELS FEATURE EXTRACTION\n",
    "### LAST TESTED OPTIMAL SEGMENTS: K=5 (no nca)\n",
    "### \"\" BOXES: K=8\n",
    "\n",
    "from object_detection_analysis.classifiers import KNNClassifier\n",
    "\n",
    "MAX_K = 25\n",
    "for k in range(1, MAX_K+1):\n",
    "    knn = KNNClassifier(n_neighbors=k, enable_nca=False, preprocess=channels_feature_preprocess)\n",
    "    knn.fit(X_train, y_train)\n",
    "    \n",
    "    print(\"_\"*82)\n",
    "    print(f\"EVALUATION: K = {k}\")\n",
    "    print(\"_\"*82)\n",
    "    knn.evaluate(X_test, y_test, disp_labels=class_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################ K VALUE TEST FOR KNN WITH RESNET FEATURE EXTRACTION ################ \n",
    "### BEST FOR RESNET18: K=5 (SEGMENT) K=3 (BOX)\n",
    "### BEST FOR RESNET34: K=6 (SEGMENT) \n",
    "### BEST FOR RESNET50: NONE\n",
    "\n",
    "from object_detection_analysis.classifiers import KNNClassifier\n",
    "\n",
    "MAX_K = 15\n",
    "for k in range(1, MAX_K+1):\n",
    "    knn = KNNClassifier(n_neighbors=k, preprocess=rn34_feature_preprocess, enable_nca=False)\n",
    "    knn.fit(X_train, y_train)\n",
    "    \n",
    "    print(\"_\"*82)\n",
    "    print(f\"EVALUATION: K = {k}\")\n",
    "    print(\"_\"*82)\n",
    "    knn.evaluate(X_test, y_test, disp_labels=class_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN FC test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from object_detection_analysis.classifiers import CNNFCClassifier\n",
    "\n",
    "cnn = CNNFCClassifier(n_classes=4, preprocess=norm_image_to_tensor)\n",
    "cnn.to(\"cuda\")\n",
    "cnn.fit(X_train, y_train, X_test, y_test, epochs=60, batch=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actual training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################\n",
    "##### TRAIN KNN WITH RESNET FEATURE EXTRACTION #####\n",
    "#####################################################\n",
    "\n",
    "from object_detection_analysis.classifiers import KNNClassifier\n",
    "\n",
    "# LEAF CLASSIFICATION: (SEGMENTS): RESNET18-K=5, RESNET34-K=6, RESNET50-K=None\n",
    "#                      (BOXES): RESNET18-K=3\n",
    "# BLOOD CELL CLASSIFICATION: K = ?\n",
    "\n",
    "resnet_knn = KNNClassifier(n_neighbors=6, preprocess=rn34_feature_preprocess)\n",
    "# resnet_knn.fit(X, y)\n",
    "resnet_knn.fit(X_train, y_train)\n",
    "resnet_knn.evaluate(X_test, y_test, disp_labels=class_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet_knn.save_state(\"knn_rn34_k6.skl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################\n",
    "##### TRAIN KNN WITH MANUAL CHANNELS FEATURE EXTRACTION #####\n",
    "#############################################################\n",
    "\n",
    "from object_detection_analysis.classifiers import KNNClassifier\n",
    "\n",
    "# LEAF CLASSIFICATION: K = 5 (SEGMENT); K = 8 (BOXES)\n",
    "# BCELL CLASSIFICATION: K = 5\n",
    "\n",
    "knn = KNNClassifier(n_neighbors=8, preprocess=channels_feature_preprocess)\n",
    "# knn.fit(X, y)\n",
    "knn.fit(X_train, y_train)\n",
    "knn.evaluate(X_test, y_test, disp_labels=class_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn.save_state(\"knn_k5.skl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################\n",
    "##### TRAIN SVM WITH RESNET FEATURE EXTRACTION #####\n",
    "####################################################\n",
    "\n",
    "# BEST WITH RESNET34\n",
    "\n",
    "from object_detection_analysis.classifiers import SVMClassifier\n",
    "\n",
    "sv = SVMClassifier(preprocess=rn50_feature_preprocess)\n",
    "# sv.fit(X, y)\n",
    "sv.fit(X_train, y_train)\n",
    "sv.evaluate(X_test, y_test, disp_labels=class_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sv.save_state(\"svm_rn34.skl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################\n",
    "##### TRAIN SVM WITH MANUAL CHANNEL FEATURE EXTRACTION #####\n",
    "############################################################\n",
    "\n",
    "from object_detection_analysis.classifiers import SVMClassifier\n",
    "\n",
    "sv = SVMClassifier(preprocess=channels_feature_preprocess)\n",
    "# sv.fit(X, y)\n",
    "sv.fit(X_train, y_train)\n",
    "sv.evaluate(X_test, y_test, disp_labels=class_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sv.save_state(\"svm.skl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################\n",
    "##### TRAIN MLP WITH RESNET FEATURE EXTRACTION #####\n",
    "####################################################\n",
    "\n",
    "#### BEST RESULTS: RESNET34\n",
    "\n",
    "from object_detection_analysis.classifiers import MLPClassifier\n",
    "\n",
    "RN = [rn18_feature_preprocess, rn34_feature_preprocess, rn50_feature_preprocess]\n",
    "RNID = [18, 34, 50]\n",
    "\n",
    "for id, func in zip(RNID, RN):\n",
    "    rn_out_features = 512 if id != 50 else 2048\n",
    "    mlp = MLPClassifier(n_features=rn_out_features, n_classes=len(class_map), preprocess=func)\n",
    "    \n",
    "    # mlp.fit(X, y, epochs=50)\n",
    "    \n",
    "    print(\"_\"*82, \"\\nRESNET:\", id, \"\\n\", \"_\"*82)\n",
    "    mlp.fit(X_train, y_train, epochs=50)\n",
    "    mlp.evaluate(X_test, y_test, disp_labels=class_labels)\n",
    "    print(\"_\"*82)\n",
    "\n",
    "    mlp.save_state(f\"mlp_rn{id}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################\n",
    "##### TRAIN MLP WITH NORMALIZED CHANNELS FEATURE EXTRACTION #####\n",
    "#################################################################\n",
    "\n",
    "from object_detection_analysis.classifiers import MLPClassifier\n",
    "\n",
    "n_features = 32*32*(3 + 3 + 1) # 32x32 leaf RGB, HSV and Gray\n",
    "mlp = MLPClassifier(n_features=n_features, n_classes=len(class_map), preprocess=norm_channels_feature_preprocess)\n",
    "\n",
    "# mlp.fit(X, y, epochs=50)\n",
    "mlp.fit(X_train, y_train, epochs=70)\n",
    "mlp.evaluate(X_test, y_test, disp_labels=class_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp.save_state(\"mlp.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN FC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################\n",
    "########## TRAIN CNN_FC WITH OBJECTS CROPS ###########\n",
    "######################################################\n",
    "\n",
    "from object_detection_analysis.classifiers import CNNFCClassifier\n",
    "\n",
    "cnn = CNNFCClassifier(n_classes=len(class_labels), preprocess=norm_image_to_tensor)\n",
    "cnn.to(\"cuda\")\n",
    "cnn.fit(X_train, y_train, X_test, y_test, epochs=10, batch=8, save_best=True)\n",
    "cnn.evaluate(X_test, y_test, disp_labels=class_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = CNNFCClassifier.from_state(\"cnn_best.pt\")\n",
    "cnn.to(\"cuda\")\n",
    "cnn.evaluate(X_test, y_test, disp_labels=class_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.save_state(\"cnn_last.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking saved states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################################################\n",
    "## CELLS BELOW ARE FOR CHECKING SAVED MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from object_detection_analysis.classifiers import KNNClassifier\n",
    "\n",
    "knn = KNNClassifier.from_state(\"/home/juliocesar/leaf-detection/checkpoints/classifiers/knn_k8_box.skl\")\n",
    "knn.evaluate(X_test, y_test, disp_labels=class_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from object_detection_analysis.classifiers import SVMClassifier\n",
    "\n",
    "svm = SVMClassifier.from_state(\"/home/juliocesar/leaf-detection/checkpoints/classifiers/svm_box.skl\")\n",
    "svm.evaluate(X_test, y_test, disp_labels=class_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from object_detection_analysis.classifiers import MLPClassifier\n",
    "\n",
    "mlp = MLPClassifier.from_state(\"/home/juliocesar/leaf-detection/checkpoints/classifiers/mlp_box.pt\")\n",
    "mlp.evaluate(X_test, y_test, disp_labels=class_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from object_detection_analysis.classifiers import MLPClassifier\n",
    "\n",
    "mlp = MLPClassifier.from_state(\"/home/juliocesar/leaf-detection/checkpoints/classifiers/mlp_rn34_box.pt\")\n",
    "mlp.evaluate(X_test, y_test, disp_labels=class_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from object_detection_analysis.classifiers import CNNFCClassifier\n",
    "\n",
    "# FOR BOX: cnn_box.pt\n",
    "# FOR SEGMENTED: cnn.pt\n",
    "# cnn = CNNFCClassifier.from_state(\"/home/juliocesar/leaf-detection/checkpoints/classifiers/cnn_box.pt\")\n",
    "cnn = CNNFCClassifier.from_state(\"cnn_best.pt\")\n",
    "cnn.to(\"cuda\")\n",
    "cnn.evaluate(X_test, y_test, disp_labels=class_labels)\n",
    "cnn = CNNFCClassifier.from_state(\"cnn_last.pt\")\n",
    "cnn.to(\"cuda\")\n",
    "cnn.evaluate(X_test, y_test, disp_labels=class_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions (Voronoi plot, Silhouette analysis)\n",
    "from typing import Tuple\n",
    "from scipy.spatial import Voronoi, voronoi_plot_2d\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "import numpy as np\n",
    "\n",
    "def plot_voronoi(centers):\n",
    "    vor = Voronoi(centers)\n",
    "    _, ax = plt.subplots(figsize=(12,8))\n",
    "    ax.scatter(centers[:,0], centers[:,1], marker='o', s=10)\n",
    "    voronoi_plot_2d(vor, ax)\n",
    "    plt.show()\n",
    "\n",
    "def silhouette_analyze(clusterer_pca: Pipeline, X: np.ndarray, n_clusters: int, sil_range: Tuple[float,float] = (-1.0,1.0)):\n",
    "    assert(sil_range[0] >= -1.0 and sil_range[1] <= 1.0)\n",
    "    # Ax1 is for silhouette plot, Ax2 is for clusters plot\n",
    "    fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(12,12))\n",
    "    # (n_clusters+1)*10 is for inserting blank space between sikhouette plots of individual clusters\n",
    "    ax1.set(\n",
    "        xlim=sil_range,\n",
    "        ylim=[0, len(X) + (n_clusters + 1) * 10],\n",
    "    )\n",
    "    \n",
    "    Xpca = clusterer_pca[:-1].fit_transform(X)\n",
    "    cluster_labels = clusterer_pca[-1].fit_predict(Xpca)\n",
    "    sil_avg = silhouette_score(X, cluster_labels)\n",
    "    print(\"_\"*82)\n",
    "    print(f\"\\t\\tn_clusters = {n_clusters} | average silhouette_score = {sil_avg}\")\n",
    "\n",
    "    sample_sil_values = silhouette_samples(X, cluster_labels)\n",
    "    y_lower = 10\n",
    "    for cluster in range(n_clusters):\n",
    "        # Aggregate silhouette scores for samples belonging to cluster i, and sort them\n",
    "        cluster_sil_values = sample_sil_values[cluster_labels == cluster]\n",
    "        cluster_sil_values.sort()\n",
    "\n",
    "        size_cluster = cluster_sil_values.shape[0]\n",
    "        y_upper = y_lower + size_cluster        \n",
    "        \n",
    "        color = cm.nipy_spectral(float(cluster) / n_clusters)\n",
    "        ax1.fill_betweenx(\n",
    "            np.arange(y_lower, y_upper),\n",
    "            0,\n",
    "            cluster_sil_values,\n",
    "            facecolor=color,\n",
    "            edgecolor=color,\n",
    "            alpha=0.7\n",
    "        )\n",
    "        ax1.text(-0.05, y_lower + 0.5 * size_cluster, str(cluster))\n",
    "\n",
    "        # y_lower for next plot\n",
    "        y_lower = y_upper + 10\n",
    "\n",
    "    ax1.set(\n",
    "        xlabel=\"Silhouette coefficient values\",\n",
    "        ylabel=\"Cluster label\",\n",
    "        yticks=[],\n",
    "        xticks=np.arange(start=sil_range[0], stop=sil_range[1]+0.2, step=0.2),\n",
    "    )\n",
    "    # color for average silhouette score\n",
    "    ax1.axvline(x=sil_avg, color=\"red\", linestyle=\"--\")\n",
    "\n",
    "    # 2nd plot with actual clusters\n",
    "    colors = cm.nipy_spectral(cluster_labels.astype(float) / n_clusters)\n",
    "    \n",
    "    ax2.scatter(\n",
    "        Xpca[:,0], Xpca[:,1], marker='.', s=30, lw=0, alpha=0.7, c=colors, edgecolor='k',\n",
    "    )\n",
    "    centers = clusterer_pca[-1].cluster_centers_\n",
    "    ax2.scatter(\n",
    "        centers[:,0],\n",
    "        centers[:,1],\n",
    "        marker='o',\n",
    "        c=\"white\",\n",
    "        alpha=1,\n",
    "        s=200,\n",
    "        edgecolor='k',\n",
    "    )\n",
    "    # Plot cluster center label on top of the point\n",
    "    for i, c in enumerate(centers):\n",
    "        ax2.scatter(c[0], c[1], marker=\"$%d$\" % i, alpha=1, s=50, edgecolor='k')\n",
    "\n",
    "    ax2.set(\n",
    "        xlabel=\"PCA feature 0\",\n",
    "        ylabel=\"PCA feature 1\",\n",
    "    )\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features from objects\n",
    "X = [obj[1] for obj in obj_data]\n",
    "rn_features = rn18_feature_preprocess(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "n_clusters = 15\n",
    "n_inertia = np.arange(2, n_clusters+1)\n",
    "inertias = []\n",
    "for n in range(2, n_clusters+1):\n",
    "    kmeans = Pipeline(steps=[\n",
    "            (\"pca\", PCA(n_components=2)),\n",
    "            (\"kmeans\", KMeans(init=\"k-means++\", n_clusters=n, n_init=10, random_state=42)),\n",
    "        ]\n",
    "    )\n",
    "    silhouette_analyze(kmeans, rn_features, n, sil_range=(-1.0,1.0))\n",
    "    inertias.append(kmeans[-1].inertia_)\n",
    "\n",
    "# Inertia plot\n",
    "fig, ax = plt.subplots(layout=\"tight\")\n",
    "ax.plot(n_inertia, inertias)\n",
    "ax.set(xlabel=\"K\", ylabel=\"Inertia\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing K-means segmentation\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "\n",
    "kmeans = KMeans(n_clusters=2, init=\"k-means++\", n_init=10, random_state=42)\n",
    "obj = obj_data[24][1]\n",
    "pixel_labels = kmeans.fit_predict(obj.reshape(obj.shape[0]*obj.shape[1], 3))\n",
    "\n",
    "segmented = cv2.cvtColor(obj, cv2.COLOR_RGB2GRAY)\n",
    "for i in range(len(pixel_labels)):\n",
    "    row = i // segmented.shape[0]\n",
    "    col = i % segmented.shape[1]\n",
    "    segmented[row,col] = pixel_labels[i]\n",
    "\n",
    "_,axs = plt.subplots(nrows=2, layout=\"tight\")\n",
    "axs[0].imshow(obj)\n",
    "axs[1].imshow(segmented, cmap=\"gray\")\n",
    "axs[0].axis(\"off\")\n",
    "axs[1].axis(\"off\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yoloanalysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
