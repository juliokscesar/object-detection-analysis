{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%set_env CUDA_VISIBLE_DEVICES=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification models training\n",
    "\n",
    "Use this notebook to train classification models (KNN, SVM, etc) on leaf color classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data gathering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use pre-annotated dataset with each leaf segmentation and class\n",
    "# From the data.yaml of this dataset, the label number to corresponding class is:\n",
    "# 0=dark, 1=dead, 2=light, 3=medium\n",
    "\n",
    "# This creates a list called 'obj_data', which containg every object as a tuple...\n",
    "# ...containing (obj_classnum, obj_crop)\n",
    "\n",
    "from typing import List\n",
    "import scg_detection_tools.utils.image_tools as imtools\n",
    "import scg_detection_tools.utils.cvt as cvt\n",
    "from scg_detection_tools.utils.file_handling import get_all_files_from_paths, get_annotation_files\n",
    "from scg_detection_tools.dataset import read_dataset_annotation\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "IMG_DIR = \"/home/juliocesar/leaf-detection/imgs/light_group/images\"\n",
    "LBL_DIR = \"/home/juliocesar/leaf-detection/imgs/light_group/labels\"\n",
    "\n",
    "#IMG_DIR = \"/home/juliocesar/leaf-detection/imgs/hemacias/annotated/images\"\n",
    "#LBL_DIR = \"/home/juliocesar/leaf-detection/imgs/hemacias/annotated/labels\"\n",
    "\n",
    "imgs = get_all_files_from_paths(IMG_DIR, skip_ext=[\".txt\", \".json\", \".yaml\"])\n",
    "\n",
    "# CHOOSING 32x32 because of calculated average\n",
    "STANDARD_SIZE = (32, 32)\n",
    "MAX_MEDIUM_RATIO = 0.30\n",
    "\n",
    "# !!!!!! taken from data.yaml\n",
    "class_map = {0: \"dark\", 1: \"dead\", 2: \"light\", 3: \"medium\"}\n",
    "#class_map = {0: \"purple\", 1: \"white\"}\n",
    "\n",
    "def extract_objects_from_annotations(imgs: List[str], ann_dir: str, std_size=(32,32), max_cls_ratio: dict[int, float] = None, use_boxes=False):\n",
    "    img_ann = get_annotation_files(imgs, ann_dir)\n",
    "    obj_data = []\n",
    "    sample_count = {cls: 0 for cls in class_map}\n",
    "    for img in imgs:\n",
    "        ann_file = img_ann[img]\n",
    "        annotations = read_dataset_annotation(ann_file, separate_class=False)\n",
    "\n",
    "        # check if contours are boxes or segments\n",
    "        orig = cv2.imread(img)\n",
    "        orig = cv2.cvtColor(orig, cv2.COLOR_BGR2RGB)\n",
    "        imgsz = orig.shape[:2]\n",
    "\n",
    "        for ann in annotations:\n",
    "            nclass = ann[0]\n",
    "            if (max_cls_ratio is not None) and (nclass in max_cls_ratio):\n",
    "                if (len(obj_data) >= 1) and ((sample_count[nclass]/len(obj_data)) >= max_cls_ratio[nclass]):\n",
    "                    continue\n",
    "            sample_count[nclass] += 1\n",
    "\n",
    "            contour = ann[1:]\n",
    "            if use_boxes and len(contour) != 4:\n",
    "                contour = cvt.segment_to_box(contour, normalized=True, imgsz=imgsz)\n",
    "            else:\n",
    "                mask = cvt.contours_to_masks([contour], imgsz=imgsz, normalized=True)[0]\n",
    "            \n",
    "            # get only segmented object from image\n",
    "            if not use_boxes:\n",
    "                masked = orig.copy()\n",
    "                masked[mask[:,:] < 1] = 0\n",
    "\n",
    "                # crop a box around it\n",
    "                points = np.array(contour).reshape(len(contour) // 2, 2)\n",
    "                box = cvt.segment_to_box(points, normalized=True, imgsz=imgsz)\n",
    "                obj_crop = imtools.crop_box_image(masked, box)\n",
    "            else:\n",
    "                obj_crop = imtools.crop_box_image(orig, contour)\n",
    "\n",
    "            # resize to 32x32 and add to our data\n",
    "            obj_crop = cv2.resize(obj_crop, std_size, cv2.INTER_CUBIC)\n",
    "            obj_data.append((nclass, obj_crop))\n",
    "    return obj_data\n",
    "\n",
    "obj_data = extract_objects_from_annotations(imgs, LBL_DIR, STANDARD_SIZE, max_cls_ratio=None, use_boxes=False)\n",
    "ncls = [obj[0] for obj in obj_data]\n",
    "for cls in np.unique(ncls):\n",
    "    print(f\"Samples of type {cls}: {class_map[cls]!r} = {len([c for c in ncls if c == cls])}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test data loaded\n",
    "idx = 450\n",
    "plt.title(class_map[obj_data[idx][0]])\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(obj_data[idx][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split between Train and Test to evaluate model as well\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = []\n",
    "y = []\n",
    "for nclass, obj_crop in obj_data:\n",
    "    X.append(obj_crop)\n",
    "    y.append(nclass)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=0)\n",
    "class_labels = [class_map[c] for c in class_map]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing functions (to be able to call clf.predict(imgs) instead of having to extract features first and then calling clf.predict(features))\n",
    "# -> rn_feature_preprocess: use resnet feature extraction to train classificators\n",
    "# -> channels_feature_preprocess: extract RGB, HSV and Gray values from a 32x32 image as features\n",
    "def rn18_feature_preprocess(objX):\n",
    "    \"\"\"\n",
    "    Extract 512-dimensional vector of features from ResNet34\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "\n",
    "    if not isinstance(objX[0], np.ndarray):\n",
    "        raise TypeError(\"'objX' passed to preprocess function must be a list of np.ndarray RGB images\")\n",
    "\n",
    "    from object_detection_analysis.classifiers import resnet_extract_features\n",
    "    processed = []\n",
    "    for obj in objX:\n",
    "        processed.append(resnet_extract_features(obj, resnet=18))\n",
    "    return np.array(processed)\n",
    "\n",
    "def rn34_feature_preprocess(objX):\n",
    "    \"\"\"\n",
    "    Extract 512-dimensional vector of features from ResNet34\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "\n",
    "    if not isinstance(objX[0], np.ndarray):\n",
    "        raise TypeError(\"'objX' passed to preprocess function must be a list of np.ndarray RGB images\")\n",
    "\n",
    "    from object_detection_analysis.classifiers import resnet_extract_features\n",
    "    processed = []\n",
    "    for obj in objX:\n",
    "        processed.append(resnet_extract_features(obj, resnet=34))\n",
    "    return np.array(processed)\n",
    "\n",
    "def rn50_feature_preprocess(objX):\n",
    "    \"\"\"\n",
    "    Extract 512-dimensional vector of features from ResNet50\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "\n",
    "    if not isinstance(objX[0], np.ndarray):\n",
    "        raise TypeError(\"'objX' passed to preprocess function must be a list of np.ndarray RGB images\")\n",
    "\n",
    "    from object_detection_analysis.classifiers import resnet_extract_features\n",
    "    processed = []\n",
    "    for obj in objX:\n",
    "        processed.append(resnet_extract_features(obj, resnet=50))\n",
    "    return np.array(processed)\n",
    "\n",
    "def channels_feature_preprocess(objX):\n",
    "    \"\"\"\n",
    "    Extract RGB, HSV and Gray channels from objects.\n",
    "    \"\"\"\n",
    "    import cv2\n",
    "    import numpy as np\n",
    "\n",
    "    if not isinstance(objX[0], np.ndarray):\n",
    "        raise TypeError(\"'objX' passed to preprocess function must be a list of np.ndarray RGB images\")\n",
    "\n",
    "    processed = []\n",
    "    for obj in objX:\n",
    "        rgb = cv2.resize(obj, (32,32))\n",
    "        hsv = cv2.cvtColor(rgb, cv2.COLOR_RGB2HSV)\n",
    "        gray = cv2.cvtColor(rgb, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "        attributes = np.concatenate((rgb.flatten(), hsv.flatten(), gray.flatten()))\n",
    "        processed.append(attributes)\n",
    "\n",
    "    return np.array(processed)\n",
    "\n",
    "def norm_channels_feature_preprocess(objX):\n",
    "    \"\"\"\n",
    "    Extract RGB, HSV and Gray channels from objects, normalize each feature vector and then use PCA to reduce dimensionality to 256 features.\n",
    "    \"\"\"\n",
    "    import cv2\n",
    "    import numpy as np\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    from sklearn.decomposition import PCA\n",
    "    from sklearn.pipeline import Pipeline\n",
    "    \n",
    "    if not isinstance(objX[0], np.ndarray):\n",
    "        raise TypeError(\"'objX' passed to preprocess function must be a list of np.ndarray RGB images\")\n",
    "\n",
    "    processed = []\n",
    "    for obj in objX:\n",
    "        rgb = cv2.resize(obj, (32,32))\n",
    "        hsv = cv2.cvtColor(rgb, cv2.COLOR_RGB2HSV)\n",
    "        gray = cv2.cvtColor(rgb, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "        attributes = np.concatenate((rgb.flatten(), hsv.flatten(), gray.flatten()))\n",
    "        processed.append(attributes)\n",
    "    processed = np.array(processed)\n",
    "    # mean = processed.mean(axis=0, keepdims=True)\n",
    "    # std = processed.std(axis=0, keepdims=True)\n",
    "    # norm = (processed - mean) / (std + 1e-7)\n",
    "\n",
    "    norm_pca_pipe = Pipeline(\n",
    "        steps = [\n",
    "            (\"scaler\", StandardScaler()), \n",
    "            #(\"pca\", PCA(n_components=256))\n",
    "        ]\n",
    "    )\n",
    "    norm = norm_pca_pipe.fit_transform(processed)\n",
    "    return norm.astype(np.float32)\n",
    "\n",
    "\n",
    "def norm_image_to_tensor(objX):\n",
    "    \"\"\"\n",
    "    Take all images from input, transform to torch.tensor and normalize them\n",
    "    \"\"\"\n",
    "    from torchvision import transforms\n",
    "    import torch\n",
    "\n",
    "    MEAN = [0.485, 0.456, 0.406]\n",
    "    STD = [0.229, 0.224, 0.225]\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Resize((32,32)),\n",
    "        transforms.Normalize(MEAN, STD),\n",
    "    ])\n",
    "    trans = []\n",
    "    for obj in objX:\n",
    "        trans.append(transform(obj))\n",
    "    return torch.stack(trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visusalize ResNet feature extraction with PCA\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "colors=[\"red\",\"black\",\"deepskyblue\",\"green\"]\n",
    "labels=[\"dark\",\"dead\",\"light\",\"green\"]\n",
    "def test_pca(objs, tgts):\n",
    "    rn_features = rn18_feature_preprocess(objs)\n",
    "    pca = PCA(n_components=2)\n",
    "    transX = pca.fit_transform(rn_features)\n",
    "\n",
    "    fig, ax = plt.subplots(layout=\"tight\")\n",
    "    for i in range(len(transX)):\n",
    "        ax.scatter(transX[i][0],transX[i][1],c=colors[tgts[i]])\n",
    "    ax.set(xlabel=\"PCA component 0\", ylabel=\"PCA component 1\")\n",
    "    plt.show()\n",
    "\n",
    "test_pca(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters test training (e.g. optimal k value for KNN, loss for SGD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############### K VALUE TEST FOR KNN WITH MANUAL CHANNELS FEATURE EXTRACTION #################\n",
    "\n",
    "################ CHANNELS FEATURE EXTRACTION\n",
    "### LAST TESTED OPTIMAL SEGMENTS: K=5 (no nca)\n",
    "### \"\" BOXES: K=8\n",
    "\n",
    "from object_detection_analysis.classifiers import KNNClassifier\n",
    "\n",
    "MAX_K = 25\n",
    "for k in range(1, MAX_K+1):\n",
    "    knn = KNNClassifier(n_neighbors=k, enable_nca=False, preprocess=channels_feature_preprocess)\n",
    "    knn.fit(X_train, y_train)\n",
    "    \n",
    "    print(\"_\"*82)\n",
    "    print(f\"EVALUATION: K = {k}\")\n",
    "    print(\"_\"*82)\n",
    "    knn.evaluate(X_test, y_test, disp_labels=class_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################ K VALUE TEST FOR KNN WITH RESNET FEATURE EXTRACTION ################ \n",
    "### BEST FOR RESNET18: K=5 (SEGMENT) K=3 (BOX)\n",
    "### BEST FOR RESNET34: K=6 (SEGMENT) \n",
    "### BEST FOR RESNET50: NONE\n",
    "\n",
    "from object_detection_analysis.classifiers import KNNClassifier\n",
    "\n",
    "MAX_K = 15\n",
    "for k in range(1, MAX_K+1):\n",
    "    knn = KNNClassifier(n_neighbors=k, preprocess=rn34_feature_preprocess, enable_nca=False)\n",
    "    knn.fit(X_train, y_train)\n",
    "    \n",
    "    print(\"_\"*82)\n",
    "    print(f\"EVALUATION: K = {k}\")\n",
    "    print(\"_\"*82)\n",
    "    knn.evaluate(X_test, y_test, disp_labels=class_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN FC test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from object_detection_analysis.classifiers import CNNFCClassifier\n",
    "\n",
    "cnn = CNNFCClassifier(n_classes=4, preprocess=norm_image_to_tensor)\n",
    "cnn.to(\"cuda\")\n",
    "cnn.fit(X_train, y_train, X_test, y_test, epochs=60, batch=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actual training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################\n",
    "##### TRAIN KNN WITH RESNET FEATURE EXTRACTION #####\n",
    "#####################################################\n",
    "\n",
    "from object_detection_analysis.classifiers import KNNClassifier\n",
    "\n",
    "# LEAF CLASSIFICATION: (SEGMENTS): RESNET18-K=5, RESNET34-K=6, RESNET50-K=None\n",
    "#                      (BOXES): RESNET18-K=3\n",
    "# BLOOD CELL CLASSIFICATION: K = ?\n",
    "\n",
    "resnet_knn = KNNClassifier(n_neighbors=6, preprocess=rn34_feature_preprocess)\n",
    "# resnet_knn.fit(X, y)\n",
    "resnet_knn.fit(X_train, y_train)\n",
    "resnet_knn.evaluate(X_test, y_test, disp_labels=class_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet_knn.save_state(\"knn_rn34_k6.skl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################\n",
    "##### TRAIN KNN WITH MANUAL CHANNELS FEATURE EXTRACTION #####\n",
    "#############################################################\n",
    "\n",
    "from object_detection_analysis.classifiers import KNNClassifier\n",
    "\n",
    "# LEAF CLASSIFICATION: K = 5 (SEGMENT); K = 8 (BOXES)\n",
    "# BCELL CLASSIFICATION: K = 5\n",
    "\n",
    "knn = KNNClassifier(n_neighbors=8, preprocess=channels_feature_preprocess)\n",
    "# knn.fit(X, y)\n",
    "knn.fit(X_train, y_train)\n",
    "knn.evaluate(X_test, y_test, disp_labels=class_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn.save_state(\"knn_k5.skl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################\n",
    "##### TRAIN SVM WITH RESNET FEATURE EXTRACTION #####\n",
    "####################################################\n",
    "\n",
    "# BEST WITH RESNET34\n",
    "\n",
    "from object_detection_analysis.classifiers import SVMClassifier\n",
    "\n",
    "sv = SVMClassifier(preprocess=rn50_feature_preprocess)\n",
    "# sv.fit(X, y)\n",
    "sv.fit(X_train, y_train)\n",
    "sv.evaluate(X_test, y_test, disp_labels=class_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sv.save_state(\"svm_rn34.skl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################\n",
    "##### TRAIN SVM WITH MANUAL CHANNEL FEATURE EXTRACTION #####\n",
    "############################################################\n",
    "\n",
    "from object_detection_analysis.classifiers import SVMClassifier\n",
    "\n",
    "sv = SVMClassifier(preprocess=channels_feature_preprocess)\n",
    "# sv.fit(X, y)\n",
    "sv.fit(X_train, y_train)\n",
    "sv.evaluate(X_test, y_test, disp_labels=class_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sv.save_state(\"svm.skl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################\n",
    "##### TRAIN MLP WITH RESNET FEATURE EXTRACTION #####\n",
    "####################################################\n",
    "\n",
    "#### BEST RESULTS: RESNET34\n",
    "\n",
    "from object_detection_analysis.classifiers import MLPClassifier\n",
    "\n",
    "RN = [rn18_feature_preprocess, rn34_feature_preprocess, rn50_feature_preprocess]\n",
    "RNID = [18, 34, 50]\n",
    "\n",
    "for id, func in zip(RNID, RN):\n",
    "    rn_out_features = 512 if id != 50 else 2048\n",
    "    mlp = MLPClassifier(n_features=rn_out_features, n_classes=len(class_map), preprocess=func)\n",
    "    \n",
    "    # mlp.fit(X, y, epochs=50)\n",
    "    \n",
    "    print(\"_\"*82, \"\\nRESNET:\", id, \"\\n\", \"_\"*82)\n",
    "    mlp.fit(X_train, y_train, epochs=50)\n",
    "    mlp.evaluate(X_test, y_test, disp_labels=class_labels)\n",
    "    print(\"_\"*82)\n",
    "\n",
    "    mlp.save_state(f\"mlp_rn{id}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################\n",
    "##### TRAIN MLP WITH NORMALIZED CHANNELS FEATURE EXTRACTION #####\n",
    "#################################################################\n",
    "\n",
    "from object_detection_analysis.classifiers import MLPClassifier\n",
    "\n",
    "n_features = 32*32*(3 + 3 + 1) # 32x32 leaf RGB, HSV and Gray\n",
    "mlp = MLPClassifier(n_features=n_features, n_classes=len(class_map), preprocess=norm_channels_feature_preprocess)\n",
    "\n",
    "# mlp.fit(X, y, epochs=50)\n",
    "mlp.fit(X_train, y_train, epochs=70)\n",
    "mlp.evaluate(X_test, y_test, disp_labels=class_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp.save_state(\"mlp.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN FC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################\n",
    "########## TRAIN CNN_FC WITH OBJECTS CROPS ###########\n",
    "######################################################\n",
    "\n",
    "from object_detection_analysis.classifiers import CNNFCClassifier\n",
    "\n",
    "cnn = CNNFCClassifier(n_classes=len(class_labels), preprocess=norm_image_to_tensor)\n",
    "cnn.to(\"cuda\")\n",
    "cnn.fit(X_train, y_train, X_test, y_test, epochs=30, batch=8, save_best=True)\n",
    "cnn.evaluate(X_test, y_test, disp_labels=class_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.save_state(\"cnn_last.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking saved states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################################################\n",
    "## CELLS BELOW ARE FOR CHECKING SAVED MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from object_detection_analysis.classifiers import KNNClassifier\n",
    "\n",
    "knn = KNNClassifier.from_state(\"/home/juliocesar/leaf-detection/checkpoints/classifiers/knn_k8_box.skl\")\n",
    "knn.evaluate(X_test, y_test, disp_labels=class_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from object_detection_analysis.classifiers import SVMClassifier\n",
    "\n",
    "svm = SVMClassifier.from_state(\"/home/juliocesar/leaf-detection/checkpoints/classifiers/svm_box.skl\")\n",
    "svm.evaluate(X_test, y_test, disp_labels=class_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from object_detection_analysis.classifiers import MLPClassifier\n",
    "\n",
    "mlp = MLPClassifier.from_state(\"/home/juliocesar/leaf-detection/checkpoints/classifiers/mlp_box.pt\")\n",
    "mlp.evaluate(X_test, y_test, disp_labels=class_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from object_detection_analysis.classifiers import MLPClassifier\n",
    "\n",
    "mlp = MLPClassifier.from_state(\"/home/juliocesar/leaf-detection/checkpoints/classifiers/mlp_rn34_box.pt\")\n",
    "mlp.evaluate(X_test, y_test, disp_labels=class_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from object_detection_analysis.classifiers import CNNFCClassifier\n",
    "\n",
    "# FOR BOX: cnn_box.pt\n",
    "# FOR SEGMENTED: cnn.pt\n",
    "# cnn = CNNFCClassifier.from_state(\"/home/juliocesar/leaf-detection/checkpoints/classifiers/cnn_box.pt\")\n",
    "cnn = CNNFCClassifier.from_state(\"cnn_best.pt\")\n",
    "cnn.to(\"cuda\")\n",
    "cnn.evaluate(X_test, y_test, disp_labels=class_labels)\n",
    "cnn = CNNFCClassifier.from_state(\"cnn_last.pt\")\n",
    "cnn.to(\"cuda\")\n",
    "cnn.evaluate(X_test, y_test, disp_labels=class_labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yoloanalysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
